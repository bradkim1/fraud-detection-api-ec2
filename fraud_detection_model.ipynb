{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Model Training\n",
    "\n",
    "This notebook demonstrates the process of training the fraud detection model used in our API. We'll go through:\n",
    "\n",
    "1. Data loading and exploration\n",
    "2. Data preprocessing\n",
    "3. Feature engineering\n",
    "4. Model training and evaluation\n",
    "5. Model saving for use in the API\n",
    "\n",
    "The trained model is saved as a pickle file that can be loaded by the fraud detection API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Set up notebook options\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "We'll load a sample of the transaction data and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# For demonstration, we'll create a synthetic dataset\n",
    "# In a real scenario, you would load your transaction data\n",
    "\n",
    "def generate_synthetic_data(n_samples=10000, fraud_ratio=0.1):\n",
    "    \"\"\"Generate synthetic transaction data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate transaction amounts\n",
    "    legitimate_amounts = np.random.lognormal(mean=4.5, sigma=1, size=int(n_samples * (1 - fraud_ratio)))\n",
    "    fraud_amounts = np.random.lognormal(mean=6, sigma=1.5, size=int(n_samples * fraud_ratio))\n",
    "    \n",
    "    # Generate card IDs\n",
    "    legitimate_card_ids = np.random.randint(1000, 9000, size=int(n_samples * (1 - fraud_ratio)))\n",
    "    fraud_card_ids = np.random.randint(1000, 9000, size=int(n_samples * fraud_ratio))\n",
    "    \n",
    "    # Product codes\n",
    "    product_codes = ['C', 'H', 'R', 'S', 'W']\n",
    "    legitimate_products = np.random.choice(product_codes, size=int(n_samples * (1 - fraud_ratio)), p=[0.4, 0.1, 0.2, 0.25, 0.05])\n",
    "    fraud_products = np.random.choice(product_codes, size=int(n_samples * fraud_ratio), p=[0.1, 0.4, 0.1, 0.1, 0.3])\n",
    "    \n",
    "    # Card types\n",
    "    card_types = ['visa', 'mastercard', 'amex', 'discover']\n",
    "    legitimate_card_types = np.random.choice(card_types, size=int(n_samples * (1 - fraud_ratio)))\n",
    "    fraud_card_types = np.random.choice(card_types, size=int(n_samples * fraud_ratio))\n",
    "    \n",
    "    # Card categories\n",
    "    card_categories = ['debit', 'credit']\n",
    "    legitimate_card_categories = np.random.choice(card_categories, size=int(n_samples * (1 - fraud_ratio)), p=[0.6, 0.4])\n",
    "    fraud_card_categories = np.random.choice(card_categories, size=int(n_samples * fraud_ratio), p=[0.3, 0.7])\n",
    "    \n",
    "    # Email domains\n",
    "    email_domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'aol.com', 'other']\n",
    "    legitimate_emails = np.random.choice(email_domains, size=int(n_samples * (1 - fraud_ratio)), p=[0.5, 0.2, 0.2, 0.05, 0.05])\n",
    "    fraud_emails = np.random.choice(email_domains, size=int(n_samples * fraud_ratio), p=[0.2, 0.1, 0.1, 0.1, 0.5])\n",
    "    \n",
    "    # Transaction timestamps (in seconds since some epoch)\n",
    "    legitimate_timestamps = np.random.randint(1000000, 2000000, size=int(n_samples * (1 - fraud_ratio)))\n",
    "    fraud_timestamps = np.random.randint(1000000, 2000000, size=int(n_samples * fraud_ratio))\n",
    "    \n",
    "    # Additional features\n",
    "    # V-features (numeric)\n",
    "    v_cols = 10\n",
    "    legitimate_v = np.random.normal(0, 1, size=(int(n_samples * (1 - fraud_ratio)), v_cols))\n",
    "    fraud_v = np.random.normal(0.5, 1.5, size=(int(n_samples * fraud_ratio), v_cols))\n",
    "    \n",
    "    # C-features (numeric)\n",
    "    c_cols = 5\n",
    "    legitimate_c = np.random.normal(0, 1, size=(int(n_samples * (1 - fraud_ratio)), c_cols))\n",
    "    fraud_c = np.random.normal(0.5, 1.5, size=(int(n_samples * fraud_ratio), c_cols))\n",
    "    \n",
    "    # D-features (numeric)\n",
    "    d_cols = 5\n",
    "    legitimate_d = np.random.normal(0, 1, size=(int(n_samples * (1 - fraud_ratio)), d_cols))\n",
    "    fraud_d = np.random.normal(0.5, 1.5, size=(int(n_samples * fraud_ratio), d_cols))\n",
    "    \n",
    "    # M-features (categorical)\n",
    "    m_values = ['T', 'F', 'M']\n",
    "    m_cols = 5\n",
    "    legitimate_m = np.array([np.random.choice(m_values, size=int(n_samples * (1 - fraud_ratio))) for _ in range(m_cols)]).T\n",
    "    fraud_m = np.array([np.random.choice(m_values, size=int(n_samples * fraud_ratio)) for _ in range(m_cols)]).T\n",
    "    \n",
    "    # Combine legitimate and fraud data\n",
    "    data = {\n",
    "        'TransactionAmt': np.concatenate([legitimate_amounts, fraud_amounts]),\n",
    "        'TransactionDT': np.concatenate([legitimate_timestamps, fraud_timestamps]),\n",
    "        'card1': np.concatenate([legitimate_card_ids, fraud_card_ids]),\n",
    "        'ProductCD': np.concatenate([legitimate_products, fraud_products]),\n",
    "        'card4': np.concatenate([legitimate_card_types, fraud_card_types]),\n",
    "        'card6': np.concatenate([legitimate_card_categories, fraud_card_categories]),\n",
    "        'P_emaildomain': np.concatenate([legitimate_emails, fraud_emails]),\n",
    "        'isFraud': np.concatenate([np.zeros(int(n_samples * (1 - fraud_ratio))), np.ones(int(n_samples * fraud_ratio))])\n",
    "    }\n",
    "    \n",
    "    # Add V, C, D, M features\n",
    "    for i in range(v_cols):\n",
    "        data[f'V{i+1}'] = np.concatenate([legitimate_v[:, i], fraud_v[:, i]])\n",
    "    \n",
    "    for i in range(c_cols):\n",
    "        data[f'C{i+1}'] = np.concatenate([legitimate_c[:, i], fraud_c[:, i]])\n",
    "    \n",
    "    for i in range(d_cols):\n",
    "        data[f'D{i+1}'] = np.concatenate([legitimate_d[:, i], fraud_d[:, i]])\n",
    "    \n",
    "    for i in range(m_cols):\n",
    "        data[f'M{i+1}'] = np.concatenate([legitimate_m[:, i], fraud_m[:, i]])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    return df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Generate synthetic data\n",
    "transaction_data = generate_synthetic_data(n_samples=10000, fraud_ratio=0.1)\n",
    "\n",
    "# Display the first few rows\n",
    "transaction_data.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# Get basic information about the dataset\n",
    "print(f\"Dataset shape: {transaction_data.shape}\")\n",
    "print(f\"Number of fraud transactions: {transaction_data['isFraud'].sum()}\")\n",
    "print(f\"Fraud rate: {transaction_data['isFraud'].mean() * 100:.2f}%\")\n",
    "\n",
    "# Check data types and missing values\n",
    "transaction_data.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Exploration\n",
    "\n",
    "Now let's explore the data and create visualizations to understand the distribution of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Visualize transaction amount distribution by fraud status\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=transaction_data, x='TransactionAmt', hue='isFraud', \n",
    "             multiple='stack', bins=30, log_scale=True)\n",
    "plt.title('Transaction Amount Distribution by Fraud Status')\n",
    "plt.xlabel('Transaction Amount (log scale)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Visualize fraud rate by product code\n",
    "product_fraud = transaction_data.groupby('ProductCD')['isFraud'].mean().reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=product_fraud, x='ProductCD', y='isFraud')\n",
    "plt.title('Fraud Rate by Product Code')\n",
    "plt.xlabel('Product Code')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.show()\n",
    "\n",
    "# Visualize fraud rate by card type\n",
    "card_fraud = transaction_data.groupby('card4')['isFraud'].mean().reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=card_fraud, x='card4', y='isFraud')\n",
    "plt.title('Fraud Rate by Card Type')\n",
    "plt.xlabel('Card Type')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Let's create the `AdvancedMLPipeline` class that will handle feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "class AdvancedMLPipeline:\n",
    "    def __init__(self, model_type='rf', n_components=5, remove_outliers=True):\n",
    "        # Initialize all components: model, imputers, encoder, PCA\n",
    "        self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer_num = SimpleImputer(strategy='mean')\n",
    "        self.imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        self.encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.remove_outliers = remove_outliers\n",
    "        self.cat_columns = []\n",
    "        self.feature_columns = []\n",
    "\n",
    "    def _add_features(self, df):\n",
    "        # Create time features from TransactionDT\n",
    "        if 'TransactionDT' in df.columns:\n",
    "            df['hour'] = pd.to_datetime(df['TransactionDT'], unit='s', errors='coerce').dt.hour.astype('Int64')\n",
    "            df['day'] = pd.to_datetime(df['TransactionDT'], unit='s', errors='coerce').dt.dayofweek.astype('Int64')\n",
    "            df.drop(columns=['TransactionDT'], inplace=True)\n",
    "\n",
    "        # Group-level aggregation features\n",
    "        c_cols = [col for col in df.columns if col.startswith('C')]\n",
    "        d_cols = [col for col in df.columns if col.startswith('D')]\n",
    "        v_cols = [col for col in df.columns if col.startswith('V')]\n",
    "\n",
    "        if c_cols:\n",
    "            df['C_sum'] = df[c_cols].sum(axis=1)\n",
    "        if d_cols:\n",
    "            df['D_missing'] = df[d_cols].isnull().sum(axis=1)\n",
    "        if v_cols:\n",
    "            df['V_mean'] = df[v_cols].mean(axis=1)\n",
    "        return df\n",
    "\n",
    "    def _remove_outliers(self, df, col='TransactionAmt'):\n",
    "        # Remove outliers from TransactionAmt using IQR\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 1.5 * IQR\n",
    "            upper = Q3 + 1.5 * IQR\n",
    "            before = df.shape[0]\n",
    "            df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
    "            after = df.shape[0]\n",
    "            print(f\"Removed {before - after} outliers from {col}\")\n",
    "        return df\n",
    "\n",
    "    def fit_model_on_chunk(self, df, label_column='isFraud'):\n",
    "        df = self._add_features(df)\n",
    "\n",
    "        if self.remove_outliers:\n",
    "            df = self._remove_outliers(df)\n",
    "\n",
    "        # Convert known categoricals to string\n",
    "        known_cats = ['ProductCD', 'card4', 'card6', 'P_emaildomain',\n",
    "                      'M1','M2','M3','M4','M5']\n",
    "        for col in known_cats:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str)\n",
    "\n",
    "        # Identify categorical and numeric columns\n",
    "        self.cat_columns = [col for col in df.columns if df[col].dtype == 'object' and col != label_column]\n",
    "        num_cols = [col for col in df.columns if col not in self.cat_columns + [label_column]]\n",
    "\n",
    "        # Impute missing values\n",
    "        if self.cat_columns:\n",
    "            df[self.cat_columns] = self.imputer_cat.fit_transform(df[self.cat_columns])\n",
    "        if num_cols:\n",
    "            df[num_cols] = self.imputer_num.fit_transform(df[num_cols])\n",
    "\n",
    "        # One-hot encode categorical features\n",
    "        encoded = self.encoder.fit_transform(df[self.cat_columns])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=self.encoder.get_feature_names_out(self.cat_columns), index=df.index)\n",
    "\n",
    "        # Replace original categoricals with encoded version\n",
    "        df = df.drop(columns=self.cat_columns)\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "        # Scale numeric columns\n",
    "        if num_cols:\n",
    "            df[num_cols] = self.scaler.fit_transform(df[num_cols])\n",
    "\n",
    "        # Apply PCA on V-columns\n",
    "        v_cols = [col for col in df.columns if col.startswith('V')]\n",
    "        if v_cols:\n",
    "            pca_trans = self.pca.fit_transform(df[v_cols])\n",
    "            pca_df = pd.DataFrame(pca_trans, columns=[f'V_PCA_{i}' for i in range(pca_trans.shape[1])], index=df.index)\n",
    "            df = df.drop(columns=v_cols)\n",
    "            df = pd.concat([df, pca_df], axis=1)\n",
    "\n",
    "        # Save the final feature columns\n",
    "        self.feature_columns = [col for col in df.columns if col not in [label_column]]\n",
    "\n",
    "        # Train model\n",
    "        X = df[self.feature_columns]\n",
    "        y = df[label_column]\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def transform_for_predict(self, df):\n",
    "        df = self._add_features(df)\n",
    "\n",
    "        # Fill missing categorical values\n",
    "        for col in self.cat_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = \"missing\"\n",
    "        \n",
    "        if self.cat_columns:\n",
    "            df[self.cat_columns] = self.imputer_cat.transform(df[self.cat_columns])\n",
    "\n",
    "        # Impute numeric\n",
    "        num_cols = [col for col in df.columns if col not in self.cat_columns]\n",
    "        if num_cols:\n",
    "            df[num_cols] = self.imputer_num.transform(df[num_cols])\n",
    "\n",
    "        # Encode categoricals\n",
    "        encoded = self.encoder.transform(df[self.cat_columns])\n",
    "        encoded_df = pd.DataFrame(\n",
    "            encoded, \n",
    "            columns=self.encoder.get_feature_names_out(self.cat_columns), \n",
    "            index=df.index\n",
    "        )\n",
    "\n",
    "        df = df.drop(columns=self.cat_columns)\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "        # Scale numeric\n",
    "        if num_cols:\n",
    "            df[num_cols] = self.scaler.transform(df[num_cols])\n",
    "\n",
    "        # Apply PCA to V columns\n",
    "        v_cols = [col for col in df.columns if col.startswith('V')]\n",
    "        if v_cols:\n",
    "            pca_trans = self.pca.transform(df[v_cols])\n",
    "            pca_df = pd.DataFrame(\n",
    "                pca_trans, \n",
    "                columns=[f'V_PCA_{i}' for i in range(pca_trans.shape[1])], \n",
    "                index=df.index\n",
    "            )\n",
    "            df = df.drop(columns=v_cols)\n",
    "            df = pd.concat([df, pca_df], axis=1)\n",
    "\n",
    "        # Align columns with training\n",
    "        for col in self.feature_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "        df = df[self.feature_columns]\n",
    "        return df\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        y_pred = self.model.predict(X)\n",
    "        y_proba = self.model.predict_proba(X)[:, 1]\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_true, y_proba))\n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "        \n",
    "        # ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc_score(y_true, y_proba):.3f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': self.model.feature_importances_\n",
    "        })\n",
    "        feature_importance = feature_importance.sort_values('importance', ascending=False).head(15)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "        plt.title('Top 15 Feature Importances')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'y_pred': y_pred,\n",
    "            'y_proba': y_proba,\n",
    "            'auc': roc_auc_score(y_true, y_proba),\n",
    "            'feature_importance': feature_importance\n",
    "        }"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    transaction_data.drop('isFraud', axis=1),\n",
    "    transaction_data['isFraud'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=transaction_data['isFraud']\n",
    ")\n",
    "\n",
    "# Combine X_train and y_train for pipeline\n",
    "train_df = X_train.copy()\n",
    "train_df['isFraud'] = y_train\n",
    "\n",
    "# Initialize and train the pipeline\n",
    "pipeline = AdvancedMLPipeline(model_type='rf', n_components=5, remove_outliers=True)\n",
    "X_processed, y_processed = pipeline.fit_model_on_chunk(train_df, label_column='isFraud')\n",
    "\n",
    "# Evaluate the model\n",
    "X_test_processed = pipeline.transform_for_predict(X_test)\n",
    "evaluation_results = pipeline.evaluate(X_test_processed, y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Saving for Use in the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Create model directory if it doesn't exist\n",
    "MODEL_DIR = '../model'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(MODEL_DIR, 'model.pkl')\n",
    "joblib.dump(pipeline.model, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save the pipeline\n",
    "pipeline_path = os.path.join(MODEL_DIR, 'pipeline.pkl')\n",
    "joblib.dump(pipeline, pipeline_path)\n",
    "print(f\"Pipeline saved to {pipeline_path}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Prediction\n",
    "\n",
    "Let's demonstrate how to make a prediction using the saved model and pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# Example transaction data\n",
    "example_transaction = pd.DataFrame({\n",
    "    'TransactionAmt': [100.0],\n",
    "    'ProductCD': ['C'],\n",
    "    'card1': [1234],\n",
    "    'card4': ['visa'],\n",
    "    'card6': ['debit'],\n",
    "    'P_emaildomain': ['gmail.com'],\n",
    "    'TransactionDT': [1500000],\n",
    "})\n",
    "\n",
    "# Transform the data\n",
    "X_example = pipeline.transform_for_predict(example_transaction)\n",
    "\n",
    "# Make prediction\n",
    "prediction = pipeline.model.predict(X_example)[0]\n",
    "probability = pipeline.model.predict_proba(X_example)[0, 1]\n",
    "\n",
    "print(f\"Prediction: {'Fraud' if prediction == 1 else 'Legitimate'}\")\n",
    "print(f\"Fraud Probability: {probability:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "We've successfully trained a fraud detection model using a Random Forest classifier and created a preprocessing pipeline that handles:\n",
    "\n",
    "- Feature engineering\n",
    "- Missing value imputation\n",
    "- Categorical encoding\n",
    "- Numerical scaling\n",
    "- Dimensionality reduction\n",
    "\n",
    "The model and pipeline are saved as pickle files that can be loaded by our fraud detection API to make real-time predictions on new transactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
